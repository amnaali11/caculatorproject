<html>
<body>
<p>
<font face="arial" size="4">
<h1>How AI Will Impact The Future Of Work And Life?</h1>
AI, or artificial intelligence, seems to be on the tip of everyone’s tongue these days. While I’ve been aware of this major trend in tech development for a while, 
I’ve noticed AI appearing more and more as one of the most in-demand areas of expertise for job seekers.
I’m sure that for many of us, the term “AI” conjures up sci-fi fantasies or fear about robots taking over the world. The depictions of AI in the media have run the gamut, and while no one can predict exactly how it will evolve in the future, 
the current trends and developments paint a much different picture of how AI will become part of our lives. 
In reality, AI is already at work all around us, impacting everything from our search results, to our online dating prospects, to the way we shop. 
Data shows that the use of AI in many sectors of business has grown by 270% over the last four years.
But what will AI mean for the future of work? As computers and technology have evolved, this has been one of the most pressing questions. 
As with many technological developments throughout history, the advancement of artificial intelligence has created fears that human workers will become obsolete. 
AI will probably not make human workers obsolete, at least not for a long time 
To put some of your fears to bed: the robots are probably not coming for your jobs, at least not yet. 
Given how artificial intelligence has been portrayed in the media, in particular in some of our favorite sci-fi movies, it’s clear that the advent of this technology has created fear that AI will one day make human beings obsolete in the workforce. After all, as technology has advanced, many tasks that were once executed by human hands have become automated. It’s only natural to fear that the leap toward creating intelligent computers could herald the beginning of the end of work as we know it. 
But, I don’t think there is any reason to be so fatalistic. A recent paper published by the MIT Task Force on the Work of the Future entitled “Artificial Intelligence And The Future of Work,” looked closely at developments in AI and their relation to the world of work. The paper paints a more optimistic picture. 
Rather than promoting the obsolescence of human labor, the paper predicts that AI will continue to drive massive innovation that will fuel many existing industries and could have the potential to create many new sectors for growth, ultimately leading to the creation of more jobs.
While AI has made major strides toward replicating the efficacy of human intelligence in executing certain tasks, there are still major limitations. In particular, AI programs are typically only capable of “specialized” intelligence, meaning they can solve only one problem, and execute only one task at a time. Often, they can be rigid, and unable to respond to any changes in input, or perform any “thinking” outside of their prescribed programming. 
Humans, however, possess “generalized intelligence,” with the kind of problem solving, abstract thinking and critical judgement that will continue to be important in business. Human judgement will be relevant, if not in every task, then certainly throughout every level across all sectors. 
There are many other factors that could limit runaway advancement in AI. AI often requires “learning” which can involve massive amounts of data, calling into question the availability of the right kind of data, and highlighting the need for categorization and issues of privacy and security around such data. There is also the limitation of computation and processing power. The cost of electricity alone to power one supercharged language model AI was estimated at 4.6 million. 
Another important limitation of note is that data can itself carry bias, and be reflective of societal inequities or the implicit biases of the designers who create and input the data. If there is bias in the data that is inputted into an AI, this bias is likely to carry over to the results generated by the AI. 
Computers have helped us to calculate the vastness of space and the minute details of subatomic particles. 
When it comes to counting and calculating, or following logical yes/no algorithms – computers outperform humans thanks to the electrons moving through their circuitry at the speed of light. 
But we generally don’t consider them as “intelligent” because, traditionally, computers haven’t been able to do anything themselves, without being taught (programmed) by us first.
So far, even if a computer had access to all of the information in the world it couldn’t do anything “smart” with it. It could find us a picture of a cat – but only because we had told it that certain pictures contain cats. In other words, ask it to find a picture of a cat and it will return with a picture which it has been told is of a cat.
This has several implications which limit its helpfulness – not least that a large amount of human time has to be spent telling it what every picture contains. 
The data (pictures) need to pass through a human bottleneck, where they are labelled, before the computer can, with lightning-quick precision, identify it as a cat picture and show it to us when we request it.
While this works well enough if we are just searching for cat pictures on Google to pass our time, if we want to do something more advanced – such as monitor a live video feed and tell us when a cat wanders in front of the camera – it’s not so great.
It is problems like this which machine learning is trying to solve. At its most simple, machine learning is about teaching computers to learn in the same way we do, by interpreting data from the world around us, classifying it and learning from its successes and failures. In fact, machine learning is a subset, or better, the leading edge of artificial intelligence.
<br><br>
<center><img src="Difference between AI v ML.png" width="80%"></center>
<br><br>
<h1>How did machine learning come about?</h1>
Building algorithms capable of doing this, using the binary “yes” and “no” logic of computers, is the foundation of machine learning – a phrase which was probably first used during serious research by Arthur Samuel at IBM during the 1950s. Samuel’s earliest experiments involved teaching machines to learn to play checkers.
As knowledge – something to draw insight from and a basis for making decisions – is deeply integral to learning, these early computers were severely handicapped due to the lack of data at their disposal. 
Without all of the digital technology we have today to capture and store information from the analogue world, machines could only learn from data slowly inputted through punch cards and, later, magnetic tapes and storage.
Today things are a little different – thanks to the rollout of the internet, the proliferation of mobile, data-gathering phones and other devices and the adoption of online, connected technology in industry, we literally have more data than we know how to deal with.
No human brain can hope to process even a fraction of the digital information it has available. But, with its lightning speed and infallible binary logic, could a computer?
<br>
<h1>The neural net and deep learning</h1>
The idea that it can, is one half of what is driving the world-changing breakthroughs we are seeing today. The other half is the “brain” of machine learning. Because as well as simply ingesting data, a machine has to process it in order to learn.
Several different frameworks have been experimented with over the years, when building algorithms designed to let machines deal with data in the same way as humans do. These often drew from the field of statistics, employing methods such as linear regression and sampling to assign probabilities to various outcomes, therefore enabling predictions to be made.
However, the framework which has, in recent years, overtaken all others in popularity by consistently proving its usefulness and adaptability, is the artificial neural network.
By throwing neuroscience into the mix, researchers found that computer models which appear to function more similarly to a human brain than anything previously developed, were possible. Artificial neural networks, like real brains, are formed from connected “neurons”, all capable of carrying out a data-related task – such as recognizing something, failing to recognise it, matching a piece of information to another piece and answering a question about the relationship between them.
Each neuron is capable of passing on the results of its work to a neighboring neuron, which can then process it further. Because the network is capable of changing and adapting based on the data that passes through it, so as to more efficiently deal with the next bit of data it comes across, it can be thought of as “learning”, in much the same way as our brains do.
“Deep learning” – another hot topic buzzword – is simply machine learning which is derived from “deep” neural nets. These are built by layering many networks on top of each other, passing information down through a tangled web of algorithms to enable a more complex simulation of human learning. Due to the increasing power and falling price of computer processors, machines with enough grunt to run these networks are becoming increasingly affordable.
<br>
<h1>What can be done with machine learning?</h1>
The application of machine learning to society and industry is leading to advancements across many fields of human endeavour.
For example, in medicine, machine learning is being applied to genomic data to help doctors understand, and predict, how cancer spreads, meaning more effective treatments can be developed.
Data from deep space is being collected here on Earth through huge radio telescopes – and after being analyzed with machine learning, is helping us to unlock the secrets of black holes.
In retail, machine learning matches shoppers with products they want to buy online, and in the bricks ‘n’ mortar world it allows shop assistants to personalize the service they offer their customers.
In the war against terror and extremism, machine learning is used to predict the behavior of those wanting to harm the innocent.
In our day-to-day lives, machine learning now powers Google’s search and image algorithms, to more accurately match us with the information we need in our lives, at the time we need it.
The process of allowing computers to understand and communicate with us in human language, thanks to machine learning, is known as natural language processing (NLP) and this has led to breakthroughs in translation technology and the voice controlled devices we increasingly use every day, including Amazon’s Echo.
Without a doubt, machine learning is proving itself to be a technology with far-reaching transformative powers. The science fiction dream of robots capable of working alongside us and augmenting our own inventiveness and imagination with their flawless logic and superhuman speed is no longer a dream – it is becoming a reality in many fields. Machine learning is the key which has unlocked it, and its potential future applications are almost unlimited.
</p>
<br><br>
<center><img src="implementing-artificial .jpg "width="80%"height="50%"><br><br>
<img src="Artificial-Intelligence1.jpg" width="80%"height="50%"><br><br>
</center></font>
</body>
</html>
